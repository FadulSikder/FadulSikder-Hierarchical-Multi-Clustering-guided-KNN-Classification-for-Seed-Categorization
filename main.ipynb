{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finale_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, sys"
      ],
      "metadata": {
        "id": "VzjuxfKFaL5-"
      },
      "execution_count": 947,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 948,
      "metadata": {
        "id": "K8G77OOUMxwj"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    #loading data from cvs file\n",
        "    data = pd.read_csv(\"/content/sample_data/Seed_Data.csv\")\n",
        "    X = data.iloc[:, :-1]\n",
        "    Y = data.iloc[:,-1:]\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get_distance_matrix() use this to get euclidean_distance\n",
        "def get_euclidean_distance(x1, x2):\n",
        "        d = 0.0\n",
        "        for i in range(0, len(x1)):\n",
        "            d = d + (x1[i] - x2[i]) ** 2\n",
        "        return math.sqrt(d)"
      ],
      "metadata": {
        "id": "Tpkx0iuwZQ5u"
      },
      "execution_count": 949,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_distance_matrix(data):\n",
        "\n",
        "      temp_data = data.to_numpy()\n",
        "      #creating a 210x210 distance matrix with zero\n",
        "      distance_matrix = np.zeros((temp_data.shape[0],temp_data.shape[0]))\n",
        "\n",
        "      #loop to fill the distance matrix\n",
        "      for i in range(distance_matrix.shape[0]):\n",
        "          for j in range(distance_matrix.shape[0]):\n",
        "              distance_matrix[i][j] = get_euclidean_distance(temp_data[i], temp_data[j])\n",
        "\n",
        "      #filling the distance matrix diagonal with big number from zero\n",
        "      np.fill_diagonal(distance_matrix, sys.maxsize)\n",
        "      \n",
        "      return distance_matrix"
      ],
      "metadata": {
        "id": "f4xXb88fZJes"
      },
      "execution_count": 950,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_all_point(distance_matrix,cluster_algo):\n",
        "      array_clusters = {}\n",
        "      cluster_id = []\n",
        "      row_id = 0\n",
        "      col_id = 0\n",
        "\n",
        "      # Storing all datapoint position in an list and naming it cluster id \n",
        "      \n",
        "      for n in range(distance_matrix.shape[0]):\n",
        "          cluster_id.append(n)\n",
        "      \n",
        "      # #storing the first cluster id on cluster list\n",
        "      array_clusters[0] = cluster_id.copy()\n",
        "  \n",
        "      # after first cluster Creating a new cluster until we reach one single cluster\n",
        "      for k in range(1, distance_matrix.shape[0]):\n",
        "          min_val = sys.maxsize\n",
        "          \n",
        "\n",
        "          #collecting the min value from matrix and taking the position id. This position id will merge as a cluster\n",
        "          for i in range(distance_matrix.shape[0]):\n",
        "              for j in range(distance_matrix.shape[1]):\n",
        "                  if(distance_matrix[i][j] <= min_val):\n",
        "                      min_val = distance_matrix[i][j]\n",
        "                      row_id = i\n",
        "                      col_id = j\n",
        "          \n",
        "          # Update the distance matrix according to algo\n",
        "          for i in range(distance_matrix.shape[0]):\n",
        "              if(i != col_id):\n",
        "                  if(cluster_algo == 'average'):\n",
        "                      temp = 0.5*(distance_matrix[col_id][i]+distance_matrix[row_id][i])\n",
        "                  elif(cluster_algo == 'single'):\n",
        "                      temp = min(distance_matrix[col_id][i],distance_matrix[row_id][i])\n",
        "                  else:\n",
        "                      temp = max(distance_matrix[col_id][i],distance_matrix[row_id][i])\n",
        "                  \n",
        "                  # Symmetric update of distance matrix\n",
        "                  distance_matrix[col_id][i] = temp\n",
        "                  distance_matrix[i][col_id] = temp\n",
        "\n",
        "\n",
        "          #setting the two merged position's values to max\n",
        "          for i in range (distance_matrix.shape[0]):\n",
        "              distance_matrix[row_id][i] = sys.maxsize\n",
        "              distance_matrix[i][row_id] = sys.maxsize\n",
        "          \n",
        "\n",
        "          minimum = min(row_id,col_id)\n",
        "          maximum = max(row_id,col_id)\n",
        "\n",
        "\n",
        "          #generating new cluster id by encode on top of the previous cluster id  \n",
        "          for n in range(len(cluster_id)):\n",
        "              if(cluster_id[n] == maximum):\n",
        "                  cluster_id[n] = minimum\n",
        "\n",
        "\n",
        "          #storing the new cluster id on cluster list\n",
        "          array_clusters[k] = cluster_id.copy()\n",
        "\n",
        "\n",
        "      return array_clusters"
      ],
      "metadata": {
        "id": "_0tesLF1ZZpm"
      },
      "execution_count": 951,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster(data):\n",
        "      # Distance Matrix\n",
        "      distance_matrix = get_distance_matrix(data)\n",
        "\n",
        "      #Finding the clusters\n",
        "      array_clusters = cluster_all_point(distance_matrix)\n",
        "      \n",
        "      return array_clusters"
      ],
      "metadata": {
        "id": "lc_f9ZjCY4ec"
      },
      "execution_count": 952,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_in_specific_no_of_cluster(data_with_new_feature, algo_clusters,number_of_clusters):\n",
        "      \n",
        "      cluster_algo = ['average','single','complete']\n",
        "      indices_of_clusters = []\n",
        "      \n",
        "      #algo cluster store all the cluster for the three algo\n",
        "      # we will iterate through it and generate specific number of cluster\n",
        "\n",
        "      for i in range(len(algo_clusters)):\n",
        "          #print('\\033[1m'+\"Clustering algorihtm : \", cluster_algo[i],' with ', number_of_clusters,' cluster'+ '\\033[0m')\n",
        "\n",
        "          # Getting n clusters and save them backward\n",
        "          array_clusters = algo_clusters[i]\n",
        "          n = len(array_clusters) - number_of_clusters\n",
        "          cluster = array_clusters[n]\n",
        "          \n",
        "          # Getting individual cluster\n",
        "          unique_arr = np.unique(cluster)\n",
        "          n_clusters = []\n",
        "          for n in np.nditer(unique_arr):\n",
        "              n_clusters.append(np.where(cluster == n))\n",
        "\n",
        "          \n",
        "          #storing the cluster id into the feature vector\n",
        "          for j in range(len(n_clusters)):\n",
        "              #print(\"Cluster \", j + 1, \" : \", n_clusters[j][0])\n",
        "              indices = n_clusters[j][0]\n",
        "              data_with_new_feature.loc[indices,cluster_algo[i]] = j\n",
        "\n",
        "          #storing the indices of the cluster of each elgo into indices_of_clusters variable\n",
        "          indices_of_clusters.append(n_clusters)\n",
        "    \n",
        "      return indices_of_clusters, data_with_new_feature"
      ],
      "metadata": {
        "id": "8IaE5tjwZv7S"
      },
      "execution_count": 953,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creating_new_feature_with_similarity_measuement(distance_matrix, indices_of_clusters, data_with_new_feature):\n",
        "   \n",
        "    # inversing the distance matrix as we will use this matrix to vote\n",
        "    distance_matrix = 1/distance_matrix\n",
        "\n",
        "\n",
        "    #iterating through each datapoint and measuring the similarity between cluster and datapoints \n",
        "    #creating a new feature vector based on that information\n",
        "    for ind in data_with_new_feature.index:\n",
        "\n",
        "        #taking a data point of cluster ids of each datapoints\n",
        "        data_point =data_with_new_feature.loc[[ind]]\n",
        "        data_point = data_point[['average','single','complete']].values.reshape(-1)\n",
        "\n",
        "        temp2_arr = np.empty((0))\n",
        "        temp3_arr = np.arange(start=0, stop=210, step=1)\n",
        "\n",
        "        #Finding the cluster union and intersection\n",
        "        for j in range(len(data_point)):\n",
        "            temp1_arr = indices_of_clusters[j]\n",
        "            temp2_arr = np.union1d(temp2_arr, temp1_arr[data_point[j]][0])\n",
        "            temp3_arr = np.intersect1d(temp3_arr, temp1_arr[data_point[j]][0])\n",
        "\n",
        "        #calculating similarity_coficient between the multiple clusters of a data point\n",
        "        similarity_cofficient = len(temp3_arr)/len (temp2_arr)\n",
        "\n",
        "\n",
        "        #updating the feature vactor with a voting algo in place of cluster_id\n",
        "        for j in range(len(data_point)):\n",
        "            temp1_arr = indices_of_clusters[j]\n",
        "            vote = distance_matrix[ind, temp1_arr[data_point[j]][0]].sum()\n",
        "            data_with_new_feature.iloc[ind, 7 + j ] = 1 /(vote + similarity_cofficient)\n",
        "            #print()\n",
        "\n",
        "    return data_with_new_feature\n"
      ],
      "metadata": {
        "id": "ZbOdnbnjxjxL"
      },
      "execution_count": 954,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this one dimentional distance matrix is used by KNN algorithm \n",
        "def cartesian_distance(feature, converted_array_of_data_point):\n",
        "    distance_matrix = np.power((feature - converted_array_of_data_point), 2)\n",
        "    distance_matrix = distance_matrix.astype(float)\n",
        "    return np.sqrt(np.sum(distance_matrix, axis=1))"
      ],
      "metadata": {
        "id": "OMG7Bifz79Vz"
      },
      "execution_count": 955,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_algorithm(feature, labels, data_point, k):\n",
        "    #intializing variable\n",
        "    row = feature.shape[0]\n",
        "    collum = feature.shape[1]\n",
        "    weighted_vote_kama = 0\n",
        "    weighted_vote_rosa = 0\n",
        "    weighted_vote_canadian = 0\n",
        "\n",
        "    #getting cartesian distance and sorting the feature vector and separating the k nearest point\n",
        "    converted_array_of_data_point = np.full((row, collum), data_point)\n",
        "    distance = cartesian_distance(feature, converted_array_of_data_point)\n",
        "    distance_labels = np.column_stack((distance, labels))\n",
        "    distance_labels = distance_labels[np.argsort(distance_labels[:, 0])]\n",
        "    k_nearest_points = distance_labels[:k, :]\n",
        "\n",
        "    #weighted voting from nearest points to select the label for the data point\n",
        "    for i in range(k):\n",
        "        if k_nearest_points[i,1] == 0 :\n",
        "            weighted_vote_kama = weighted_vote_kama + (1/k_nearest_points[i,0])\n",
        "        elif k_nearest_points[i,1] == 1:\n",
        "            weighted_vote_rosa = weighted_vote_rosa + (1/k_nearest_points[i,0])\n",
        "        else:\n",
        "            weighted_vote_canadian = weighted_vote_canadian + (1/k_nearest_points[i,0])\n",
        "\n",
        "    #slecting the label based on highest vote\n",
        "    if (weighted_vote_kama >= weighted_vote_rosa) and (weighted_vote_kama >= weighted_vote_canadian):\n",
        "        result = 0\n",
        "    elif (weighted_vote_rosa > weighted_vote_kama) and (weighted_vote_rosa > weighted_vote_canadian):\n",
        "        result = 1\n",
        "    else:\n",
        "        result = 2\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "nxg1dZBX74Z6"
      },
      "execution_count": 956,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leave_one_out_evaluation(k, data_with_new_feature):\n",
        "\n",
        "    loop_var = data_with_new_feature.shape[0]\n",
        "    error_count = 0\n",
        "\n",
        "    #itareting through feature vector\n",
        "    for i in range(loop_var):\n",
        "        #droping row\n",
        "        dropped_row = data_with_new_feature.iloc[[i], :]\n",
        "        data = data_with_new_feature.drop(data_with_new_feature.index[i])\n",
        "        \n",
        "        #creating new feature and labels after droping one point\n",
        "        feature = data.iloc[:, :-1].values\n",
        "        labels = data.iloc[:,-1:].to_numpy()\n",
        "\n",
        "        #formating the droped data point\n",
        "        data_point = dropped_row.iloc[:, :-1].to_numpy()\n",
        "        expected_prediction = dropped_row.iloc[:,-1:].to_numpy().flatten()\n",
        "        \n",
        "        #sending the droped data point and rest feature and labels to perform kNN classification\n",
        "        result = knn_algorithm(feature, labels, data_point, k)\n",
        "\n",
        "        #counting error\n",
        "        if result != expected_prediction:\n",
        "            error_count = error_count + 1\n",
        "\n",
        "    #calculating accuracy\n",
        "    percent_of_error = error_count / (loop_var-1) * 100\n",
        "    prediction_accuracy = 100 - percent_of_error\n",
        "\n",
        "    return prediction_accuracy"
      ],
      "metadata": {
        "id": "qzTRv_bB7yVX"
      },
      "execution_count": 957,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    #intailizing variables\n",
        "    data,labels = load_data()\n",
        "    data_with_new_feature = data.copy()\n",
        "    algo_clusters = []\n",
        "    data_with_new_feature['average'] = ''\n",
        "    data_with_new_feature['single'] = ''\n",
        "    data_with_new_feature['complete'] = ''\n",
        "\n",
        "    #getting distance matrix for performing clustering\n",
        "    distance_matrix = get_distance_matrix(data)\n",
        "    #performing hierarchical clustering and collecting the clustering result\n",
        "    for cluster_algo in ['average','single','complete']:\n",
        "        distance_matrix_copy = distance_matrix.copy()\n",
        "        array_clusters = cluster_all_point(distance_matrix_copy,cluster_algo)\n",
        "        algo_clusters.append(array_clusters)\n",
        "    #[4,6,8,9,10,11,12,16,18,21,24,27,30,60]\n",
        "    for number_of_clusters in [4,6,8,12,16,20,24,27,30,60]:\n",
        "\n",
        "        #dividing the clusters into specific cluster number and adding cluster label on each datapoint of feature vector\n",
        "        indices_of_clusters, data_with_new_feature = divide_in_specific_no_of_cluster(data_with_new_feature, algo_clusters, number_of_clusters)\n",
        "        \n",
        "        #creating new feature vector considering the similarity beatween clusters and data point \n",
        "        data_with_new_feature = creating_new_feature_with_similarity_measuement(distance_matrix,indices_of_clusters, data_with_new_feature)\n",
        "\n",
        "        data_with_new_feature = pd.concat([data_with_new_feature, labels], axis=1, join='inner')\n",
        "        accuracy = [0,0,0]\n",
        "        j = 0\n",
        "\n",
        "        for k in [3,5,7]:\n",
        "          accuracy[j] = leave_one_out_evaluation(k, data_with_new_feature)\n",
        "          print('\\033[1m'+'Prediction accuracy of species when Cluster Number is ',number_of_clusters,'and K is ', k, ' ==>', accuracy[j],'%'+ '\\033[0m')\n",
        "          j = j + 1\n",
        "\n",
        "    data = pd.concat([data, labels], axis=1, join='inner')\n",
        "    accuracy2 = [0,0,0]\n",
        "    j = 0\n",
        "    for k in [3,5,7]:\n",
        "          accuracy2[j] = leave_one_out_evaluation(k, data)\n",
        "          print('\\033[1m'+'Prediction accuracy of species without cluster feature and K is ', k, ' ==>', accuracy2[j],'%'+ '\\033[0m')\n",
        "          j = j + 1\n",
        "    return"
      ],
      "metadata": {
        "id": "9mFWvRwmYhkh"
      },
      "execution_count": 958,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rngN7-SFYluc",
        "outputId": "c11b9d3c-82f5-451e-9ec6-c023a24b8d45"
      },
      "execution_count": 959,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mPrediction accuracy of species when Cluster Number is  4 and K is  3  ==> 88.99521531100478 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  4 and K is  5  ==> 88.03827751196172 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  4 and K is  7  ==> 88.03827751196172 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  6 and K is  3  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  6 and K is  5  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  6 and K is  7  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  8 and K is  3  ==> 99.04306220095694 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  8 and K is  5  ==> 99.04306220095694 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  8 and K is  7  ==> 99.04306220095694 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  12 and K is  3  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  12 and K is  5  ==> 97.12918660287082 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  12 and K is  7  ==> 98.08612440191388 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  16 and K is  3  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  16 and K is  5  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  16 and K is  7  ==> 99.04306220095694 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  20 and K is  3  ==> 99.04306220095694 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  20 and K is  5  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  20 and K is  7  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  24 and K is  3  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  24 and K is  5  ==> 100.0 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  24 and K is  7  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  27 and K is  3  ==> 100.0 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  27 and K is  5  ==> 100.0 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  27 and K is  7  ==> 100.0 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  30 and K is  3  ==> 99.04306220095694 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  30 and K is  5  ==> 98.56459330143541 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  30 and K is  7  ==> 98.08612440191388 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  60 and K is  3  ==> 99.52153110047847 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  60 and K is  5  ==> 100.0 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species when Cluster Number is  60 and K is  7  ==> 100.0 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species without cluster feature and K is  3  ==> 88.99521531100478 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species without cluster feature and K is  5  ==> 88.03827751196172 %\u001b[0m\n",
            "\u001b[1mPrediction accuracy of species without cluster feature and K is  7  ==> 88.03827751196172 %\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}